"""Pressure is a module of PROFFASTpylot.

Hand the pressure to PROFFAST, add own functions to handle different
data formats.

License information:
PROFFASTpylot - Running PROFFAST with Python
Copyright (C)   2022    Lena Feld, Benedikt Herkommer,
                        Karlsruhe Institut of Technology (KIT)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License version 3 as published by
the Free Software Foundation.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import pandas as pd
import datetime as dt
import glob
import os
import sys
import numpy as np
import yaml


class PressureHandler():
    """Read, interpolate and return pressure data from various formats."""

    mandatory_options = [
        "dataframe_parameters",
        "filename_parameters",
        "data_parameters",
        "frequency"
    ]

    default_options = {
        "utc_offset": 0.0,
        "pressure_factor": 1.0
    }

    parsed_dtcol = "parsed_datetime"

    def __init__(self, pressure_type_file, pressure_path, dates, logger):
        """
        Initialize the Pressure Handler.
        Params:
            pressure_type_file(str): path to the pressure config file
            pressure_path(str): path to the folder where the pressure files
                                are located.
            dates(list of datetime-objects): A list of all days supposed to
                                                be processed. Use the same list
                                                as generated by the
                                                PROFFASTpylot
        """
        self.dates = dates
        self.logger = logger
        self.pressure_path = pressure_path

        with open(pressure_type_file, "r") as f:
            args = yaml.load(f, Loader=yaml.FullLoader)
        for option, value in args.items():
            self.__dict__[option] = value

        for option, default in self.default_options.items():
            if self.__dict__.get(option) is None:
                self.__dict__[option] = default
                self.logger.debug(
                    f"The optional option {option} was set to the default "
                    f"value: {default}.")

        for option in self.mandatory_options:
            if self.__dict__.get(option) is None:
                self.logger.critical(
                    f"Mandatory option {option} not given in the pressure type"
                    f" file {pressure_type_file}!")
                sys.exit()

        self.p_df = pd.DataFrame()

    def prepare_pressure_df(self):
        """Read the pressure of a day, from files with a various frequencies.

        The dataframe self.p_df is created as a object of the pressure_handler
        instance Containing the pressure and a datetime column.

        The pressure column is multiplied by the pressure_factor given in the 
        pressure input file.

        """
        self.logger.debug("Execute prepare_pressure_df()...")
        frequency = self.frequency

        # Create the p_df for different file frequencies
        if frequency == "subdaily":
            self._read_subdaily_files()
        elif frequency == "daily":
            self._read_subdaily_files()
        elif frequency == "weekly":
            raise NotImplementedError(
                f"{frequency} frequency not implemented yet.")
        elif frequency == "monthly":
            raise NotImplementedError(
                f"{frequency} frequency not implemented yet.")
        elif frequency == "yearly":
            self._read_yearly_files()
        elif frequency == "unregular":
            self._read_unregular_files()
        else:
            raise ValueError(f"Unknown frequency {frequency}.")

        self._multiply_pressure_factor()
        # Reset index to let in be unique
        self.p_df.reset_index(drop=True, inplace=True)

    def get_pressure_at(self, timestamp):
        """ Return the pressure at timestamp
        params:
            timestamp (datetime)
        """
        pkey = self.dataframe_parameters["pressure_key"]
        # get the two closest entries:
        # calculate differences to current value:
        diff = (self.p_df[self.parsed_dtcol] - timestamp).dt.total_seconds()
        diff = abs(diff).sort_values()
        inds = diff.index[:2].to_list()
        inds.sort()
        i1 = inds[0]
        i2 = inds[1]
        t1 = self.p_df.loc[i1][self.parsed_dtcol]
        t2 = self.p_df.loc[i2][self.parsed_dtcol]
        if not (t1 < timestamp and t2 > timestamp):
            if i1 == 0 or i2 == len(self.p_df) - 1:
                # at the beginning of the dataseries, data will be extrapolated
                self.logger.warning(
                    f"No pressure data available for {timestamp}."
                    "Pressure data will be linear extrapolated!")
                # print("==============")
                # print("IN GET PRESSURE: USING DATAFRAME:", self.p_df)
                # print(f"timestamp: {timestamp}")
                # print(self.p_df.loc[i1-1:i2+1])
                # print(f"i1: {i1}, i2: {i2}")
                # print(f"t1: {t1}, t2: {t2}")
                # print(
                #   f"p1: {self.p_df.loc[i1][pkey]},"
                #   f"p2: {self.p_df.loc[i2][pkey]}")
                # print("===============")
            else:
                # for not equistant data this case can happen
                if t2 < timestamp:
                    i2 += 1
                    i1 += 1
                if t1 > timestamp:
                    i1 -= 1
                    i2 -= 1
                t1 = self.p_df.loc[i1][self.parsed_dtcol]
                t2 = self.p_df.loc[i2][self.parsed_dtcol]
        if abs((t2 - t1).total_seconds()) / 3600 > 6:
            self.logger.warning(
                "Pressure is interpolated for a time range larger than 6 h "
                f"for date {timestamp}. This might give wrong results!"
                "Please check the input data for this day."
            )
        m = (self.p_df.loc[i2][pkey] - self.p_df.loc[i1][pkey])\
            / abs((t2 - t1).total_seconds())

        if np.isnan(m):
            m = 0
            self.logger.warning(
                "There was unknown Error whilst interpolating the pressure "
                f"for datetime {timestamp}."
                "Take the non inpterpolated nearest neighbour instead"
            )

        p = m * \
            (timestamp - t1).total_seconds()\
            + self.p_df.loc[i1][pkey]
        return p

    def _read_subdaily_files(self):
        """Reads the subdaily AND daily files into the internal p_df
        """
        for day in self.dates:

            daily_df = pd.DataFrame()
            filename = self._get_filename(day)
            dataloggerFileList = glob.glob(
                os.path.join(self.pressure_path, filename))
            # print("Files to read in: ", dataloggerFileList)
            dataloggerFileList.sort()
            # get all files of one day and concat them:
            for file in dataloggerFileList:
                # print(f"Read in file {file}")
                temp = pd.read_csv(
                    file,
                    **(self.dataframe_parameters["csv_kwargs"]))
                daily_df = pd.concat([daily_df, temp])
            daily_df = self._parse_datetime_col(daily_df, day)
            self.p_df = pd.concat([self.p_df, daily_df])
        
        self.p_df.reset_index(drop=True, inplace=True)
        self._parse_pressure()

    def _read_yearly_files(self):
        """read yearly files and return a dict containing the pressure
        for each day in dates
        """
        first_year = self.dates[0].year
        last_year = self.dates[-1].year
        if first_year == last_year:
            years = [first_year]
        else:
            years = np.arange(first_year, last_year + 1)
        # read in all needed years:
        df = pd.DataFrame()
        for year in years:
            filename = self._get_filename(
                dt.datetime(year=year, month=1, day=1))
            fileList = glob.glob(
                os.path.join(self.pressure_path, filename))
            if len(fileList) > 1:
                raise RuntimeError("Found more than one yearly pressure file")
            if len(fileList) == 0:
                raise RuntimeError("Could not find a pressure file")
            temp = pd.read_csv(
                fileList[0],
                **(self.dataframe_parameters["csv_kwargs"]))
            df = pd.concat([df, temp])
        df = self._parse_datetime_col(df)
        self.p_df = df
        self._parse_pressure()

    def _read_unregular_files(self):
        """read unregular files. Save the result in self.p_df DataFrame"""
        params = self.filename_parameters
        filename = "".join([params["basename"], "*", params["ending"]])
        file_list = glob.glob(os.path.join(self.pressure_path, filename))        

        df = pd.DataFrame()
        for file in file_list:
            temp = pd.read_csv(
                file, **(self.dataframe_parameters["csv_kwargs"]))
            df = pd.concat([df, temp])
        df = self._parse_datetime_col(df)
        self.p_df = df
        self._parse_pressure()

    def _parse_pressure(self, date=None):
        """
        Parse the internal raw p_df and eliminate bad values
        """
        df_args = self.dataframe_parameters
        p_key = df_args["pressure_key"]
        # Filter values which are too large or too small.
        # Replace or remove them.
        maxVal = float(self.data_parameters["max_pressure"])
        minVal = float(self.data_parameters["min_pressure"])
        replace_val = 0
        if self.data_parameters["default_value"] == "skip":
            replace_val = np.nan
        else:
            replace_val = float(self.data_parameters["default_value"])
        self.p_df[p_key] = np.where(
            self.p_df[p_key] > maxVal, replace_val, self.p_df[p_key])
        self.p_df[p_key] = np.where(
            self.p_df[p_key] < minVal, replace_val, self.p_df[p_key])
        self.p_df = self.p_df.dropna(subset=[p_key])

    def _multiply_pressure_factor(self):
        """Multiply the pressure column with the pressure factor."""
        pressure_key = self.dataframe_parameters["pressure_key"]
        self.p_df[pressure_key] *= self.pressure_factor

    def _parse_datetime_col(self, df, date=None):
        """
        parse the dataframe for a suitable datetime.
        Add the column 'parsed_datecol' to the dataframe
        """
        if len(df) == 0:
            self.logger.warning(
                f"For date {date} an empty dataset is read in!")
            return df
        df_args = self.dataframe_parameters
        time_key = df_args["time_key"]
        time_fmt = df_args["time_fmt"]
        date_key = df_args["date_key"]
        date_fmt = df_args["date_fmt"]
        dt_key = df_args["datetime_key"]
        dt_fmt = df_args["datetime_fmt"]

        if time_key != "" and dt_key != "":
            raise RuntimeError(
                "time_key and datetime_key can not be given at the same time")

        if dt_key == "":
            # no datetime column available, check for date column:
            if date_key == "":
                # no date key avaliable as well. Do only take the time from
                # file.
                # day is taken from call day
                try:
                    df[self.parsed_dtcol] = pd.to_datetime(
                        df[time_key], format=time_fmt)
                except KeyError:
                    self.logger.critical(
                        f"Could not access key {time_key} in pressure data."
                        "Exit Program.")
                    exit()

                df[self.parsed_dtcol] = df[self.parsed_dtcol].apply(
                    lambda x: x.replace(
                        day=date.day, month=date.month, year=date.year))
            else:
                # combine two columns to datetime
                try:
                    df[self.parsed_dtcol] = pd.to_datetime(
                        df[date_key] + df[time_key],
                        format=date_fmt+time_fmt)
                except KeyError:
                    self.logger.critical(
                        f"Could not find key {date_key} or {time_key} in "
                        f"pressure data for date {date}. Exit Program.")
                    self.logger.debug(f"The dataframe is: {df}")
                    exit()
        else:
            # seems that a datetime column is available:
            try:
                df[self.parsed_dtcol] = pd.to_datetime(
                    df[dt_key], format=dt_fmt)
            except KeyError:
                self.logger.critical(
                    f"Could not fine key {dt_key} in "
                    "pressure data. Exit Program.")
                exit()

        return df

    def _get_filename(self, date):
        """Return merged filename of pressure_type."""
        params = self.filename_parameters
        filename = "".join(
                [params["basename"],
                    date.strftime(params["time_format"]),
                    params["ending"]]
            )
        return filename
