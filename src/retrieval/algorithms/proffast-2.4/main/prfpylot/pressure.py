"""Pressure is a module of PROFFASTpylot.

Hand the pressure to PROFFAST, add own functions to handle different
data formats.

License information:
PROFFASTpylot - Running PROFFAST with Python
Copyright (C)   2022    Lena Feld, Benedikt Herkommer,
                        Karlsruhe Institut of Technology (KIT)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License version 3 as published by
the Free Software Foundation.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

import pandas as pd
import datetime as dt
import glob
import os
import sys
import numpy as np
import yaml
import copy


class PressureHandler():
    """Read, interpolate and return pressure data from various formats."""

    mandatory_options = [
        "dataframe_parameters",
        "filename_parameters",
        "data_parameters",
        "frequency"
    ]

    default_options = {
        "utc_offset": 0.0,
        "pressure_factor": 1.0,
        "pressure_offset": 0.0,
        "data_parameters": {},
        "max_interpolation_time": 2,  # in hours
    }

    parsed_dtcol = "parsed_datetime"

    def __init__(
            self, pressure_type_file, pressure_path, dates, logger,
            measurement_time=0):
        """
        Initialize the Pressure Handler.
        Parameters:
            pressure_type_file(str): path to the pressure config file
            pressure_path(str): path to the folder where the pressure files
                                are located.
            dates(list of datetime-objects): A list of all days supposed to
                                                be processed. Use the same list
                                                as generated by the
                                                PROFFASTpylot
        """
        # make a deepcopy of the dates since we want to add one day at the end
        # and the beginning of the list. And since lists are mutable in python
        # therefore are kind of `called by reference`, it is necessary to
        # explicitly deepcopy it here.
        self.dates = copy.deepcopy(dates)
        self.logger = logger
        self.pressure_path = pressure_path
        
        self.interpolation_failed_at = []

        with open(pressure_type_file, "r") as f:
            args = yaml.load(f, Loader=yaml.FullLoader)
        for option, value in args.items():
            self.__dict__[option] = value

        for option, default in self.default_options.items():
            if self.__dict__.get(option) is None:
                self.__dict__[option] = default
                self.logger.debug(
                    f"The pressure parameter {option} was set to the default "
                    f"value: {default}.")

        for option in self.mandatory_options:
            if self.__dict__.get(option) is None:
                self.logger.critical(
                    f"Mandatory pressure parameter {option} not given in the "
                    f"pressure type file {pressure_type_file}!")
                sys.exit()
            else:
                # fill defaults and check for missing values inside the dicts
                self.__dict__[option] = self._set_defaults(option)
        self._check_mandatory()

        # For a later read in of the pressure data frame it makes sense to only
        # read in the columns needed. In case of large meteo files, this can
        # save a lot of RAM and processing time.
        self.cols_to_use = []
        for key in ["pressure_key", "time_key", "date_key", "datetime_key"]:
            val = self.dataframe_parameters[key]
            if val != "":
                self.cols_to_use.append(val)

        # to ensure the date-columns are read in as string format
        self.dataframe_parameters["csv_kwargs"] = \
            self._append_dtype_to_csv_kwargs()

        # When the pressure data is recorded using a different time zone, than
        # the FTIR data, this can lead to gaps in the data. Hence convert the
        # date list to a datelist which matches with the pressure data
        # time zone.
        if abs(measurement_time - self.utc_offset) > 4:
            temp = self.dates.copy()
            for date in temp:
                # measurement_time = UTC + UTC_offset_measurement
                # p_time = UTC + UTC_offset_p
                # Hence, if measurement_time > UTC_offset_p then we also need
                # to load the date BEFORE the current pressure date:
                if measurement_time > self.utc_offset:
                    previous_day = date - dt.timedelta(days=1)
                    self.dates.append(previous_day)
                # when the measurement_time < UTC_offset_p then we also need
                # to load the date AFTER the current pressure date
                else:
                    next_day = date + dt.timedelta(days=1)
                    self.dates.append(next_day)
            # delete the duplicate days:
            self.dates = list(set(self.dates))
            self.dates.sort()
        self.logger.debug(
            "Dates to load pressure files for:"
            "\n".join([x.strftime("%Y-%m-%d") for x in self.dates]))

        self.p_df = pd.DataFrame()

    def prepare_pressure_df(self):
        """Read the pressure of a day, from files with a various frequencies.

        The dataframe self.p_df is created as a object of the pressure_handler
        instance Containing the pressure and a datetime column.

        The pressure column is multiplied by the pressure_factor given in the 
        pressure input file.

        """
        self.logger.debug("Execute prepare_pressure_df()...")
        frequency = self.frequency

        # Create the p_df for different file frequencies
        if frequency in ["subdaily", "daily"]:
            self._read_subdaily_files()
        elif frequency == "yearly":
            self._read_yearly_files()
        elif frequency == "unregular":
            self._read_unregular_files()
        elif frequency in ["monthly", "weekly"]:
            self.logger.warning(
                "Please use 'unregular' frequency."
                " weekly and monthly are not yet implemented seperately.")
            self._read_unregular_files()
        else:
            raise ValueError(f"Unknown frequency {frequency}.")

        self._apply_pressure_offset_and_factor()

        # sort values (needed for correct interpolation)
        self.p_df.sort_values(self.parsed_dtcol)

        # Reset index to let in be unique
        self.p_df.reset_index(drop=True, inplace=True)

        # print df.head for debug purposes
        df_args = self.dataframe_parameters
        p_key = df_args["pressure_key"]
        df_print = self.p_df[[self.parsed_dtcol, p_key]]
        self.logger.debug(
            "Created pressure DataFrame:\n\n"
            "# start of self.p_df #\n"
            f"{df_print.head()}\n"
            )

    def get_pressure_at(self, pressure_time):
        """Return the interpolated pressure at a given time.

        If the value is rejected or an interpolation error occured p=0
        is returned.The corresponding spectra will not be processed.
        This is determined in prepare.get_spectra_pT_input().

        If the pressure measurements for a whole day is missing, the whole
        day is deleted from the processing list in pylot.run_inv().

        Parameters:
            pressure_time (datetime:datetime):
                time in timezone of the pressure file
        """
        tkey = self.parsed_dtcol
        pkey = self.dataframe_parameters["pressure_key"]

        # reject if time difference to closed value is greater than threshhold
        # get the two closest entry by calculating differences to current value
        diff = \
            (self.p_df[tkey] - pressure_time).dt.total_seconds()
        diff = abs(diff).sort_values()
        i_nearest = diff.index[0]  # sort the two nearest to the top

        t_nearest = self.p_df.loc[i_nearest][tkey]
        threshold = self.max_interpolation_time * 3600

        if abs((t_nearest - pressure_time).total_seconds()) > threshold:
            self.logger.debug(
                f"Interpolation time for requested time {pressure_time} "
                "was larger than the threshold. Will skip the processing "
                "of the spectra corresponding to this time. "
                "(See next message!)")
            return 0

        p = np.interp(
            np.datetime64(pressure_time, "ns"),
            self.p_df[tkey].astype("datetime64[ns]"),
            self.p_df[pkey].values)

        return p

    def _read_subdaily_files(self):
        """Reads the subdaily AND daily files into the internal p_df
        """
        for day in self.dates:

            daily_df = pd.DataFrame()
            filename = self._get_filename(day)
            file_list = glob.glob(
                os.path.join(self.pressure_path, filename))
            # print("Files to read in: ", dataloggerFileList)
            if len(file_list) == 0:
                # no pressure file is available for this day!
                self.logger.warning(
                    f"No pressure file could be found at day {day}.")
            else:
                # get all files of one day and concat them:
                file_list.sort()
                for file in file_list:
                    self.logger.debug(f"Read in file {file}")
                    temp = pd.read_csv(
                        file,
                        usecols=self.cols_to_use,
                        **(self.dataframe_parameters["csv_kwargs"]))
                    daily_df = pd.concat([daily_df, temp])
            
            daily_df = self._parse_datetime_col(daily_df, day)
            self.p_df = pd.concat([self.p_df, daily_df])
        self.p_df.reset_index(drop=True, inplace=True)
        self._parse_pressure()

    def _read_yearly_files(self):
        """read yearly files and return a dict containing the pressure
        for each day in dates
        """
        first_year = self.dates[0].year
        last_year = self.dates[-1].year
        if first_year == last_year:
            years = [first_year]
        else:
            years = np.arange(first_year, last_year + 1)
        # read in all needed years:
        df = pd.DataFrame()
        for year in years:
            filename = self._get_filename(
                dt.datetime(year=year, month=1, day=1))
            fileList = glob.glob(
                os.path.join(self.pressure_path, filename))
            if len(fileList) > 1:
                raise RuntimeError("Found more than one yearly pressure file")
            if len(fileList) == 0:
                raise RuntimeError("Could not find a pressure file")
            temp = pd.read_csv(
                fileList[0],
                usecols=self.cols_to_use,
                **(self.dataframe_parameters["csv_kwargs"]))
            df = pd.concat([df, temp])
        df = self._parse_datetime_col(df)
        self.p_df = df
        self._parse_pressure()

    def _read_unregular_files(self):
        """read unregular files. Save the result in self.p_df DataFrame"""
        params = self.filename_parameters
        filename = "".join([params["basename"], "*", params["ending"]])
        file_list = glob.glob(os.path.join(self.pressure_path, filename))        

        df = pd.DataFrame()
        if len(file_list) == 0:
            self.logger.critical(
                f"No pressure data could be found in {self.presure_path}! "
                "Terminating PROFFASTpylot.")
            exit()
        for file in file_list:
            temp = pd.read_csv(
                file,
                usecols=self.cols_to_use,
                **(self.dataframe_parameters["csv_kwargs"]))
            df = pd.concat([df, temp])
        df = self._parse_datetime_col(df)
        self.p_df = df
        self._parse_pressure()

    def _parse_pressure(self, date=None):
        """
        Parse the internal raw p_df and eliminate bad values
        """
        df_args = self.dataframe_parameters
        p_key = df_args["pressure_key"]
        # Filter values which are too large or too small.
        # Replace or remove them.
        maxVal = float(self.data_parameters["max_pressure"])
        minVal = float(self.data_parameters["min_pressure"])
        replace_val = 0
        if self.data_parameters["default_value"] == "skip":
            replace_val = np.nan
        else:
            replace_val = float(self.data_parameters["default_value"])
        self.p_df[p_key] = np.where(
            self.p_df[p_key] > maxVal, replace_val, self.p_df[p_key])
        self.p_df[p_key] = np.where(
            self.p_df[p_key] < minVal, replace_val, self.p_df[p_key])
        self.p_df = self.p_df.dropna(subset=[p_key])

    def _apply_pressure_offset_and_factor(self):
        """Multiply the pressure column with the pressure factor."""
        pressure_key = self.dataframe_parameters["pressure_key"]
        self.p_df[pressure_key] *= self.pressure_factor
        self.p_df[pressure_key] += self.pressure_offset

    def _parse_datetime_col(self, df, date=None):
        """Parse the dataframe for a suitable datetime.

        Add the column 'parsed_datecol' to the dataframe
        Depending on the options given, the datetime column is constructed f
        rom the combination of the separate time and date columns.

        Parameters:
            df (pandas.DataFrame): pressure dataframe containing time 
                information in arbitrary format.

        Returns:
            df (pandas.DataFrame): with an additional datetime column.

        """
        # give warning if an empty df is read in and this is not the first
        # or the last day of the list
        # (since these are only to catch dateline issues)
        if len(df) == 0:
            if not (date == self.dates[0] or date == self.dates[-1]):
                self.logger.warning(
                    f"For date {date} an empty dataset is read in!")
                return df
            else:
                return df
        df_args = self.dataframe_parameters
        time_key = df_args["time_key"]
        time_fmt = df_args["time_fmt"]
        date_key = df_args["date_key"]
        date_fmt = df_args["date_fmt"]
        dt_key = df_args["datetime_key"]
        dt_fmt = df_args["datetime_fmt"]

        if dt_key == "":
            # no datetime column available, check for date column:
            if date_key == "":
                # no date key avaliable as well. Do only take the time from
                # file.
                # day is taken from call day
                try:
                    df[self.parsed_dtcol] = pd.to_datetime(
                        df[time_key], format=time_fmt)
                except KeyError:
                    self.logger.critical(
                        f"Could not access key {time_key} in pressure data."
                        "Exit Program.")
                    exit()

                df[self.parsed_dtcol] = df[self.parsed_dtcol].apply(
                    lambda x: x.replace(
                        day=date.day, month=date.month, year=date.year))
            else:
                # combine two columns to datetime
                try:
                    df[self.parsed_dtcol] = pd.to_datetime(
                        df[date_key] + df[time_key],
                        format=date_fmt+time_fmt)
                except KeyError:
                    self.logger.critical(
                        f"Could not find key {date_key} or {time_key} in "
                        f"pressure data for date {date}. Exit Program.")
                    self.logger.debug(f"The dataframe is: {df}")
                    exit()
        else:
            # seems that a datetime column is available:
            try:
                if dt_fmt == "POSIX-timestamp":
                    df[self.parsed_dtcol] = df[dt_key].apply(
                        lambda x: dt.datetime.utcfromtimestamp(np.float64(x)))
                else:
                    df[self.parsed_dtcol] = pd.to_datetime(
                        df[dt_key], format=dt_fmt)
            except KeyError:
                self.logger.critical(
                    f"Could not find key {dt_key} in pressure data."
                    f"Pressure data are:\n{df}\n."
                    f"Pressure folder is: {self.pressure_path}\n"
                    "Exit Program.")
                exit()

        return df

    def _get_filename(self, date):
        """Return merged filename of pressure_type."""
        params = self.filename_parameters
        filename = "".join(
                [params["basename"],
                    date.strftime(params["time_format"]),
                    params["ending"]]
            )
        return filename

    def _set_defaults(self, option):
        """Set defaults in dataframe, filename and data parameters dict.
        Check for mandatory options.

        Parameters:
            option (str): "dataframe_parameters", "filename_parameters"
                or "data_parameters"

        Return:
            modified dict
        """
        defaults = {}
        defaults["data_parameters"] = {
            "max_pressure": 1500,
            "min_pressure": 500,
            "default_value": "skip"
        }
        defaults["dataframe_parameters"] = {
            "pressure_key": "",
            "time_key": "",
            "time_fmt": "",
            "date_key": "",
            "date_fmt": "",
            "datetime_key": "",
            "datetime_fmt": "",
            "csv_kwargs": {},
        }
        defaults["filename_parameters"] = {
            "basename": "",
            "time_format": "",
            "ending": ""
        }

        d = self.__dict__[option]
        if option not in defaults.keys():
            return d
        for k, v in defaults[option].items():
            if d.get(k) is None:
                d[k] = v
                self.logger.debug(
                    f"The pressure parameter {option}:{k} was set to "
                    f"default value: {v}.")
        return d

    def _check_mandatory(self):
        """Check mandatory options for completeness.

        The options must satisfy the following:
        - A pressure key is given,
        - the time key XOR datetime key is given and
        - the filename is not empty.

        Raises:
            RuntimeError: in case of a missing option.

        """
        # pressure key are given
        pressure_key = self.dataframe_parameters.get("pressure_key")
        if pressure_key == "":
            raise RuntimeError(
                "The key of the pressure column in the pressure file must be "
                "given as dataframe_parameters: pressure_key")

        # time or datetime key
        time_key = self.dataframe_parameters.get("time_key")
        datetime_key = self.dataframe_parameters.get("datetime_key")
        general_instruction = (
            " Give either the time_key or the datetime_key in "
            "dataframe_parameters!")
        if (time_key == "") and (datetime_key == ""):
            raise RuntimeError(
                "None of time_key and datetime_key are given!"
                + general_instruction)
        elif (time_key != "") and (datetime_key != ""):
            raise RuntimeError(
                "time_key and datetime_key can not be given at the same time!"
                + general_instruction)

        # filename not empty
        joined_filename = "".join([
            self.filename_parameters["basename"],
            self.filename_parameters["time_format"],
            self.filename_parameters["ending"]
        ])
        if joined_filename == "":
            raise RuntimeError(
                "No filename is given! Give the start, time format (optional) "
                "and ending of your filename as filename_parameters: "
                "basename, time_format and ending.")

    def _append_dtype_to_csv_kwargs(self):
        """Return extended csv_kwargs to make sure the date and time column
        are interpreted as string."""
        csv_kwargs = self.dataframe_parameters["csv_kwargs"]
        dtype = {
            self.dataframe_parameters["date_key"]: str,
            self.dataframe_parameters["time_key"]: str,
            self.dataframe_parameters["datetime_key"]: str,
        }
        dtype.pop("", None)  # remove default empty string

        csv_kwargs["dtype"] = dtype
        return csv_kwargs
