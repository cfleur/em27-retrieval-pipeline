
# Using the pipeline

## Setting up the Pipeline

**Get the code**

1. Clone the repository or download the source code of a specific release.
2. Create a virtual environment using `python3.11 -m venv .venv`
3. Always ctivate the virtual environment using `source .venv/bin/activate`
4. Install the dependencies using `pip install ".[dev]"` or `pdm sync --group dev`

**Configure it on your system**

5. Use the `config/config.template.json` to create a `config.json` file.
6. Set up the metadata locally or remotely (see [metadata guide](/guides/metadata))

**Run the tests to ensure everything is working**

7. Run the tests using `pytest -m "integration or quick or ci or complete" --verbose --exitfirst tests/`

**Celebrate if the tests pass**

8. Run `curl parrot.live`

## Downloading Atmospheric Profiles

This pipeline automated the way of obtaining atmospheric profiles described in
https://tccon-wiki.caltech.edu/Main/ObtainingGinputData. You can simply add the email
address, you used to access the FTP server to `config.profiles.server.email` and configure
the scope you want to download in `config.profiles.scope`.

Run the following command to download all the profiles:

```bash
python cli.py profiles run
```

This script will use the metadata and the configured local profiles directory to determine,
which profiles to request. When calling the script once, it will request all missing profiles.
When calling the script again, it will check, whether the requests have been fulfilled since
the last call.

During this process, it ensures that only `config.profiles.server.max_parallel_requests` are
running at the same time. It only request the same profiles again, if they have not been generated
within 24 hours. The script can download partial query results (e.g. when only day 1 to 5 of a
7-day request could be fulfilled).

You can use `config.profiles.GGG2020_standard_sites` to configure a list of standard sites
you want to download. The script will never request profiles for these standard sites but only
downloads the pregenerated data.

Run the following to request the current queue status of your account:

```bash
python cli.py profiles request-ginput-status
```

## Running Retrievals

Use the following commands to start the retrievals in a background process.

```bash
python cli.py retrieval start
```

You can limit the number of cores used by the retrieval process using `config.retrievals.general.max_process_count`.

Using the following commands, you can check whether the retrievals are still
running and open a dashboard to monitor the progress.

```bash
python cli.py retrieval is-running
python cli.py retrieval watch
```

Terminate the ongoing retrievals using the following command:

```bash
python cli.py retrieval stop
```

## Exporting Campaign Datasets

Export the campaign datasets using the following command:

```bash
python cli.py export run
```

## Generate a Data Report

You can generate a report about the data on your system using the following command:

```bash
python cli.py data-report
```

This will produce one CSV file per sensor id in the directory `data/reports/`:

```csv
from_datetime,to_datetime,location_id,interferograms,datalogger,ggg2014_profiles,ggg2014_proffast_10_outputs,ggg2014_proffast_22_outputs,ggg2014_proffast_23_outputs,ggg2020_profiles,ggg2020_proffast_22_outputs,ggg2020_proffast_23_outputs
2023-09-07T00:00:00+0000,2023-09-07T23:59:59+0000,   TUM_I, 2224, 1440,✅,-,✅,✅,✅,-,✅
2023-09-08T00:00:00+0000,2023-09-08T23:59:59+0000,   TUM_I, 2178, 1440,✅,-,✅,✅,✅,-,✅
2023-09-09T00:00:00+0000,2023-09-09T23:59:59+0000,   TUM_I, 1966, 1440,✅,-,✅,✅,✅,-,✅
2023-09-10T00:00:00+0000,2023-09-10T23:59:59+0000,   TUM_I, 2034, 1440,✅,-,✅,✅,✅,-,✅
2023-09-11T00:00:00+0000,2023-09-11T23:59:59+0000,   TUM_I, 2122, 1440,✅,-,✅,✅,✅,-,✅
2023-09-12T00:00:00+0000,2023-09-12T23:59:59+0000,   TUM_I, 1972, 1440,✅,-,✅,✅,✅,-,✅
2023-09-13T00:00:00+0000,2023-09-13T23:59:59+0000,   TUM_I,  216, 1439,✅,-,✅,✅,✅,-,✅
2023-09-14T00:00:00+0000,2023-09-14T23:59:59+0000,   TUM_I,  762, 1440,✅,-,✅,✅,✅,-,✅
2023-09-15T00:00:00+0000,2023-09-15T23:59:59+0000,   TUM_I, 1507, 1440,✅,-,✅,✅,✅,-,✅
2023-09-16T00:00:00+0000,2023-09-16T23:59:59+0000,   TUM_I, 2232, 1440,✅,-,✅,✅,✅,-,✅
2023-09-17T00:00:00+0000,2023-09-17T23:59:59+0000,   TUM_I, 1599, 1440,✅,-,✅,✅,✅,-,✅
2023-09-18T00:00:00+0000,2023-09-18T23:59:59+0000,   TUM_I,  228, 1440,✅,-,✅,✅,✅,-,✅
```

The numbers in the columns "interferograms" and "datalogger" are the number of interferograms
and the number of datalogger lines for the respective day.
