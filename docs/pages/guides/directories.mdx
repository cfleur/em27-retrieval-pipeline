import { Callout } from "nextra/components";

# Directory Structure

The pipeline and none of the retrieval algorithms manipulate any data in the directories `config.general.data.datalogger`, `config.general.data.atmospheric_profiles`, and `config.general.data.interferograms`. The retrieval only writes to `config.general.data.results`.

<Callout type="info" emoji="ðŸ’¡">

We can ensure this pipeline doesn't modify any data from the input directories. We cannot guarantee this for the retrieval algorithms, but we are very confident about this and have never observed it on our systems.

However, if you want to ensure the retrieval cannot manipulate any of your interferograms or datalogger files, you can create a read-only mount of the directories. Only point the pipeline to those read-only directories.

See https://askubuntu.com/a/243390.

</Callout>

### Interferograms

Point the `config.general.data.interferograms` variable to the directory your interferograms are stored in.

```
ðŸ“‚ <config.general.data.interferograms>
+--- ðŸ“‚ ma
|    +--- ðŸ“‚ 20220101
|    |    +--- ðŸ“„ ma20210101.ifg.001
|    |    +--- ðŸ“„ ma20210101.ifg.002
|    |    +--- ðŸ“„ ma20210101.ifg.003
|    |    +--- ...
|    +--- ðŸ“‚ 20220101
|    |    +--- ðŸ“„ ma20210102.ifg.001
|    |    +--- ðŸ“„ ma20210102.ifg.002
|    |    +--- ðŸ“„ ma20210102.ifg.003
|    |    +--- ...
|    +--- ðŸ“‚ ...
+--- ðŸ“‚ mb
|    +--- ðŸ“‚ 20220101
|    +--- ðŸ“‚ ...
+--- ðŸ“‚ ...
```

In this example, `ma`, `mb`, and so on are the "sensor ids" used by us (see the [next section about metadata](/guides/metadata)).

You must set the `config.retrieval.general.ifg_file_regex` value to a regex matching your files. In the example above, we can use `^$(SENSOR_ID)$(DATE).*ifg.\\d+$`.

### Datalogger Files

Point the `config.general.data.datalogger` variable to the directory where you store your local meteorological files = your ground pressure logs.

```
ðŸ“‚ <config.general.data.datalogger>
+--- ðŸ“‚ ma
|    +--- ðŸ“„ datalogger-ma-20210101.csv
|    +--- ðŸ“„ datalogger-ma-20210102.csv
|    +--- ðŸ“„ ...
+--- ðŸ“‚ mb
|    +--- ðŸ“„ datalogger-mb-20210101.csv
|    +--- ðŸ“„ ...
+--- ðŸ“‚ ...
```

Each file looks like the following. It can have as many columns as you want, as long as you include the `UTCtime___` and `BaroYoung` columns.

```csv
UTCtime___,BaroYoung
00:00:20,942.350
00:01:20,942.35
00:02:20,942.33
00:03:20,942.3
00:04:20,942.29
```

<Callout type="info" emoji="ðŸ’¡">

Since the format of the local meteorological is less standardized than the interferograms, we decided to support this format - for now.

If your data is in a different format than this, write a small parsing script that runs on a cronjob and creates a new directory in this format. One year of data in this format is about 9MB, so the data duplication is acceptable.

We are also happy to accept contributions to our codebase to add support for different formats! Please open an issue to discuss first.

</Callout>

### Atmospheric Profiles

Point the `config.general.data.atmospheric_profiles` variable to the directory you want to store the atmospheric profiles.

```
ðŸ“‚ <config.general.data.atmospheric_profiles>
+--- ðŸ“‚ GGG2014
|    +--- ðŸ“„ 20210101_48N011E.map
|    +--- ðŸ“„ 20210101_48N011E.mod
|    +--- ðŸ“„ 20210101_48N012E.map
|    +--- ðŸ“„ 20210101_48N012E.mod
|    +--- ðŸ“„ 20210102_48N011E.map
|    +--- ðŸ“„ ...
+--- ðŸ“‚ GGG2020
     +--- ðŸ“„ 2021010100_48N011E.map
     +--- ðŸ“„ 2021010100_48N011E.mod
     +--- ðŸ“„ 2021010100_48N011E.vmr
     +--- ðŸ“„ 2021010100_48N012E.map
     +--- ðŸ“„ 2021010100_48N012E.mod
     +--- ðŸ“„ 2021010100_48N012E.vmr
     +--- ðŸ“„ 2021010103_48N011E.map
     +--- ðŸ“„ ...
```

<Callout type="info" emoji="ðŸ’¡">

Since this pipeline includes a fully automated downloader for this data, you can point it to an empty directory and let the pipeline populate it.

</Callout>

### Results

The pipeline populates the results directory in the following way:

```
ðŸ“‚ <config.general.data.results>
+--- ðŸ“‚ proffast-2.3/GGG2020
|    +--- ðŸ“‚ ma
|    |    +--- ðŸ“‚ failed
|    |    +--- ðŸ“‚ successful
|    |         +--- ðŸ“‚ 20210101
|    |         |    +--- ðŸ“‚ input_files
|    |         |    |    +--- ðŸ“„ invers20ma_210101_a.inp
|    |         |    |    +--- ðŸ“„ pcxs20ma_210101.inp
|    |         |    |    +--- ðŸ“„ preprocess5ma_210101.inp
|    |         |    +--- ðŸ“‚ logfiles
|    |         |    |    +--- ðŸ“„ container.log
|    |         |    |    +--- ðŸ“„ inv_output.log
|    |         |    |    +--- ðŸ“„ pcxs_output.log
|    |         |    |    +--- ðŸ“„ preprocess_output.log
|    |         |    |    +--- ðŸ“„ pylot_38218.log
|    |         |    +--- ðŸ“„ comb_invparms_ma_SN061_210101-210101.csv
|    |         |    +--- ðŸ“„ about.json
|    |         |    +--- ðŸ“„ pylot_config.yml
|    |         |    +--- ðŸ“„ pylot_log_format.yml
|    |         |    +--- ðŸ“„ ... (more files depending on retrieval algorithm)
|    +--- ðŸ“‚ mb
|         +--- ðŸ“‚ failed
|         +--- ðŸ“‚ successful
+--- ðŸ“‚ proffast-2.3/GGG2014
+--- ðŸ“‚ ...
```

The `about.json` file in each successful retrieval directory contains all information required to reproduce the respective retrieval results. The structure of the directories in `failed/` and `successful/` is the same - the outputs are moved to `successful/` if the retrieval has produced a final CSV file and to `failed` otherwise.

### Bundles

With `config.bundles`, you can specify a list of bundles to produce from the raw retrieval results. The script will generate one bundle per sensor, retrieval algorithm, and atmospheric profile. For example, when using the following bundle config:

```json
{
  "dst_dir": "/some/path/where/the/bundle_should/be/written/to",
  "output_formats": ["csv", "parquet"],
  "from_datetime": "2024-05-10T00:00:00+0000",
  "to_datetime": "2024-07-09T23:59:59+0000",
  "retrieval_algorithms": ["proffast-2.2", "proffast-2.4"],
  "atmospheric_profile_models": ["GGG2020"],
  "sensor_ids": ["ma", "mb"]
}
```

... the following bundles will be generated in the output directory `/some/path/where/the/bundle_should/be/written/to`:

```
ðŸ“‚ /some/path/where/the/bundle_should/be/written/to
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-ma-proffast-2.2-GGG2020-20240510-20240709.csv
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-ma-proffast-2.2-GGG2020-20240510-20240709.parquet
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-ma-proffast-2.4-GGG2020-20240510-20240709.csv
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-ma-proffast-2.4-GGG2020-20240510-20240709.parquet
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-mb-proffast-2.2-GGG2020-20240510-20240709.csv
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-mb-proffast-2.2-GGG2020-20240510-20240709.parquet
â”œâ”€â”€â”€ ðŸ“„ em27-retrieval-bundle-mb-proffast-2.4-GGG2020-20240510-20240709.csv
â””â”€â”€â”€ ðŸ“„ em27-retrieval-bundle-mb-proffast-2.4-GGG2020-20240510-20240709.parquet
```

The output files will include all data from the `<config.general.data.results>` path that matches the time period. The CSV and Parquet files contain the same data - just in two different tabular formats. They keep all columns from the raw retrieval algorithm but add four more columns `utc`, `retrieval_time`, `location_id` and `campaign_ids`:

- `utc`: parsed from the `UTC`/`HHMMSS_ID` columns to have a consistent timestamp format
- `retrieval_time`: the timestamp when the retrieval was finished
- `location_id`: the location ID of the sensor at that time
- `campaign_ids`: the campaign IDs that match this datapoint separated by a `+` sign

Proffast 1.0 bundle example:

```csv
utc,HHMMSS_ID,ground_pressure,lat,lon,alt,sza,azi,XH2O,XAIR,XCO2,XCH4,XCH4_S5P,XCO,retrieval_time,location_id,campaign_ids
2022-06-02T05:13:49.000000+0000,51349.0,998.2,48.148,16.438,180.0,70.1,-101.45,3316.9,1.00387,418.077,1.8772,0.0,0.0,2024-09-11T22:48:42.000000+0000,ZEN,both+only-mc
2022-06-02T05:14:04.000000+0000,51404.0,998.2,48.148,16.438,180.0,70.06,-101.41,3317.72,1.00343,417.989,1.87669,0.0,0.0,2024-09-11T22:48:42.000000+0000,ZEN,both+only-mc
2022-06-02T05:14:19.000000+0000,51419.0,998.2,48.148,16.438,180.0,70.02,-101.37,3317.16,1.00421,417.361,1.87585,0.0,0.0,2024-09-11T22:48:42.000000+0000,ZEN,both+only-mc
...
```

Proffast 2.4 bundle example:

```csv
utc,spectrum,ground_pressure,lat,lon,alt,sza,azi,XH2O,XAIR,XCO2,XCH4,XCO2_STR,XCO,XCH4_S5P,H2O,O2,CO2,CH4,CO,CH4_S5P,retrieval_time,location_id,campaign_ids
2022-06-02T05:13:55.000000+0000,220602_051349SN.BIN,998.2,48.148,16.438,180.0,70.1,-101.45,3435.8,0.998586,420.051,1.88495,0.0,0.0,0.0,7.24389e26,4.46289e28,8.89103e25,4.01976e23,0.0,0.0,2024-09-11T22:50:05.000000+0000,ZEN,both+only-mc
2022-06-02T05:14:09.000000+0000,220602_051404SN.BIN,998.19,48.148,16.438,180.0,70.06,-101.41,3436.61,0.998166,419.96,1.88445,0.0,0.0,0.0,7.24253e26,4.46095e28,8.88534e25,4.01701e23,0.0,0.0,2024-09-11T22:50:05.000000+0000,ZEN,both+only-mc
2022-06-02T05:14:24.000000+0000,220602_051419SN.BIN,998.19,48.148,16.438,180.0,70.02,-101.37,3435.96,0.998954,419.327,1.88353,0.0,0.0,0.0,7.24683e26,4.46442e28,8.87892e25,4.01823e23,0.0,0.0,2024-09-11T22:50:05.000000+0000,ZEN,both+only-mc
...
```

Filtering by Campaign ID can be done with one line of code:

```python
import polars as pl

df = pl.read_parquet()

df = df.filter(pl.col("campaign_ids").str.split("+").list.contains("muccnet"))
```

<Callout type="info" emoji="ðŸ’¡">

With our MUCCnet campaign config:

```json
{
  "campaign_id": "muccnet",
  "from_datetime": "2019-09-13T00:00:00+0000",
  "to_datetime": "2100-01-01T23:59:59+0000",
  "sensor_ids": ["ma", "mb", "mc", "md", "me"],
  "location_ids": ["TUM_I", "FEL", "GRAE", "OBE", "TAU", "DLR_2", "DLR_3"]
}
```

... the dataframe filtered by the campaign ID `muccnet` will only contain dat that has been generated between 2019-09-13 and 2100-01-01 by the sensors `ma`, `mb`, `mc`, `md`, and `me` at the locations `TUM_I`, `FEL`, `GRAE`, `OBE`, `TAU`, `DLR_2`, and `DLR_3`.

</Callout>

### Logs

The logs are stored within the directory of the pipeline at `data/logs`:

```
ðŸ“‚ data
+--- ðŸ“‚ logs
     +--- ðŸ“‚ retrieval
          +--- ðŸ“„ 20240106-23-54_main.log
          +--- ðŸ“„ 20240106-22-55_generous-easley.log
          +--- ðŸ“„ 20240106-23-10_eloquent-oppenheimer.log
          +--- ðŸ“‚ archive
               +--- ðŸ“‚ container (old container logs)
               +--- ðŸ“‚ main (old main logs)
```

The files are either from containers (`startingdate-startingtime_containername.log`) or from the main process (`startingdate-startingtime_main.log`), which orchestrates the containers.

### Internal

#### Containers

The containers in which the retrieval is running are working on `data/containers`. Each container with a container name like `eloquent-oppenheimer` has three active directories: `data/containers/retrieval-container-$containername`, `data/containers/retrieval-container-$containername-input`, and `data/containers/retrieval-container-$containername-output`.

#### Profiles Query Cache

The profiles downloader uses the file `data/profiles_query_cache.json` to save the information on which profiles have already been requested. Profiles will only be re-requested if they have not been produced within 24 hours.
